<section>
  <h2>Description</h2>
  <p>Welcome to the Complete AI &amp; LLM Engineering Bootcamp – your one-stop course to learn Python, Git, Docker, Pydantic, LLMs, Agents, RAG, LangChain, LangGraph, and Multi-Modal AI from the ground up.</p>
  <p>This is not just another theory course. By the end, you will be able to code, deploy, and scale real-world AI applications that use the same techniques powering ChatGPT, Gemini, and Claude.</p>
</section>

<section>
  <h2>What You’ll Learn</h2>

  <h3>Foundations</h3>
  <ul>
    <li>Python programming from scratch — syntax, data types, OOP, and advanced features.</li>
    <li>Git &amp; GitHub essentials — branching, merging, collaboration, and professional workflows.</li>
    <li>Docker — containerization, images, volumes, and deploying applications like a pro.</li>
    <li>Pydantic — type-safe, structured data handling for modern Python apps.</li>
  </ul>

  <h3>AI Fundamentals</h3>
  <ul>
    <li>What are LLMs and how GPT works under the hood.</li>
    <li>Tokenization, embeddings, attention, and transformers explained simply.</li>
    <li>Understanding multi-head attention, positional encodings, and the &quot;Attention is All You Need&quot; paper.</li>
  </ul>

  <h3>Prompt Engineering</h3>
  <ul>
    <li>Master prompting strategies: zero-shot, one-shot, few-shot, chain-of-thought, persona-based prompts.</li>
    <li>Using Alpaca, ChatML, and LLaMA-2 formats.</li>
    <li>Designing prompts for structured outputs with Pydantic.</li>
  </ul>

  <h3>Running &amp; Using LLMs</h3>
  <ul>
    <li>Setting up OpenAI &amp; Gemini APIs with Python.</li>
    <li>Running models locally with Ollama + Docker.</li>
    <li>Using Hugging Face models and INSTRUCT-tuned models.</li>
    <li>Connecting LLMs to FastAPI endpoints.</li>
  </ul>

  <h3>Agents &amp; RAG Systems</h3>
  <ul>
    <li>Build your first AI Agent from scratch.</li>
    <li>CLI-based coding agents with Claude.</li>
    <li>The complete RAG pipeline — indexing, retrieval, and answering.</li>
    <li>LangChain: document loaders, splitters, retrievers, and vector stores.</li>
    <li>Advanced RAG with Redis/Valkey Queues for async processing.</li>
    <li>Scaling RAG with workers and FastAPI.</li>
  </ul>

  <h3>LangGraph &amp; Memory</h3>
  <ul>
    <li>Introduction to LangGraph — state, nodes, edges, and graph-based AI.</li>
    <li>Adding checkpointing with MongoDB.</li>
    <li>Memory systems: short-term, long-term, episodic, semantic memory.</li>
    <li>Implementing memory layers with Mem0 and Vector DB.</li>
    <li>Graph memory with Neo4j and Cypher queries.</li>
  </ul>

  <h3>Conversational &amp; Multi-Modal AI</h3>
  <ul>
    <li>Build voice-based conversational agents.</li>
    <li>Integrate speech-to-text (STT) and text-to-speech (TTS).</li>
    <li>Code your own AI voice assistant for coding (Cursor IDE clone).</li>
    <li>Multi-modal LLMs: process images and text together.</li>
  </ul>

  <h3>Model Context Protocol (MCP)</h3>
  <ul>
    <li>What is MCP and why it matters for AI apps.</li>
    <li>MCP transports: STDIO and SSE.</li>
    <li>Coding an MCP server with Python.</li>
  </ul>
</section>

<section>
  <h2>Real-World Projects You’ll Build</h2>
  <ul>
    <li>Tokenizer from scratch.</li>
    <li>Local Ollama + FastAPI AI app.</li>
    <li>Python CLI-based coding assistant.</li>
    <li>Document RAG pipeline with LangChain &amp; Vector DB.</li>
    <li>Queue-based scalable RAG system with Redis &amp; FastAPI.</li>
    <li>AI conversational voice agent (STT + GPT + TTS).</li>
    <li>Graph memory agent with Neo4j.</li>
    <li>MCP-powered AI server.</li>
  </ul>
</section>
